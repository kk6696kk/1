# maploc/models/learnable_refinement.py

"""
Learnable End-to-End Pose Refinement Module
å¯å­¦ä¹ çš„ç«¯åˆ°ç«¯ä½å§¿ä¼˜åŒ–æ¨¡å—
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class ConfidencePredictor(nn.Module):
    """
    é¢„æµ‹æ˜¯å¦åº”è¯¥è¿›è¡Œrefinementçš„ç½®ä¿¡åº¦ç½‘ç»œ
    åŸºäºå½“å‰çš„featuresã€poseå’Œprobability distributioné¢„æµ‹
    """
    def __init__(self, feature_dim=8):
        super().__init__()
        
        # è¾“å…¥: featureç»Ÿè®¡é‡(3*C) + poseç‰¹å¾(4) + probç‰¹å¾(2)
        input_dim = feature_dim * 3 + 6
        
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, features, pose, prob_distribution):
        """
        Args:
            features: [B, C, H, W] ç‰¹å¾å›¾
            pose: [B, 4, 4] é¢„æµ‹çš„pose
            prob_distribution: [B, N_samples] æŠ•ç¥¨æ¦‚ç‡åˆ†å¸ƒ
        
        Returns:
            confidence: [B, 1] ç½®ä¿¡åº¦åˆ†æ•° [0, 1]
        """
        B = features.shape[0]
        
        # 1. Featureç»Ÿè®¡é‡
        feat_mean = features.mean(dim=(2, 3))  # [B, C]
        feat_std = features.std(dim=(2, 3))    # [B, C]
        feat_max = features.flatten(2).max(dim=2)[0]  # [B, C]
        
        # 2. Poseç‰¹å¾
        t_norm = torch.norm(pose[:, :3, 3], dim=-1, keepdim=True)  # [B, 1]
        R = pose[:, :3, :3]
        # æ—‹è½¬çŸ©é˜µçš„Frobenius norm
        R_norm = torch.norm(R.reshape(B, -1), dim=-1, keepdim=True)  # [B, 1]
        # æ—‹è½¬çŸ©é˜µçš„trace (æ¥è¿‘3è¡¨ç¤ºæ¥è¿‘å•ä½çŸ©é˜µ)
        trace = torch.diagonal(R, dim1=-2, dim2=-1).sum(-1, keepdim=True)  # [B, 1]
        # Determinant (åº”è¯¥æ¥è¿‘1)
        det = torch.det(R).unsqueeze(-1)  # [B, 1]
        
        # 3. Probability distributionç‰¹å¾
        prob_max = prob_distribution.max(dim=-1, keepdim=True)[0]  # [B, 1]
        prob_entropy = -(prob_distribution * torch.log(prob_distribution + 1e-8)).sum(-1, keepdim=True)  # [B, 1]
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        x = torch.cat([
            feat_mean, feat_std, feat_max,
            t_norm, R_norm, trace, det,
            prob_max, prob_entropy
        ], dim=-1)  # [B, input_dim]
        
        confidence = self.fc(x)  # [B, 1]
        return confidence


class AdaptiveDampingNetwork(nn.Module):
    """
    è‡ªé€‚åº”å­¦ä¹ æ¯ä¸ªå‚æ•°çš„dampingå› å­
    """
    def __init__(self, num_levels=3):
        super().__init__()
        self.num_levels = num_levels
        
        # æ¯ä¸ªlevelä¸€ä¸ªdampingé¢„æµ‹ç½‘ç»œ
        self.damping_nets = nn.ModuleList([
            nn.Sequential(
                nn.Linear(6, 32),  # è¾“å…¥ï¼š6-DoFæ¢¯åº¦çš„ç»Ÿè®¡é‡
                nn.ReLU(),
                nn.Linear(32, 16),
                nn.ReLU(),
                nn.Linear(16, 6),
                nn.Softplus()  # ç¡®ä¿è¾“å‡ºæ­£å€¼
            )
            for _ in range(num_levels)
        ])
        
        # æ¯ä¸ªlevelçš„åŸºç¡€damping (å¯å­¦ä¹ )
        self.base_log_damping = nn.ParameterList([
            nn.Parameter(torch.zeros(6))  # logç©ºé—´ï¼Œåˆå§‹åŒ–ä¸º1.0
            for _ in range(num_levels)
        ])
    
    def forward(self, level, gradient_stats):
        """
        Args:
            level: int, å½“å‰stageç´¢å¼•
            gradient_stats: [B, 6] æ¢¯åº¦çš„ç»Ÿè®¡é‡ï¼ˆé€šå¸¸æ˜¯ç»å¯¹å€¼æˆ–å¹³æ–¹ï¼‰
        
        Returns:
            damping: [B, 6] æ¯ä¸ªå‚æ•°çš„dampingå› å­
        """
        # åŸºç¡€damping (logç©ºé—´å­¦ä¹ æ›´ç¨³å®š)
        base_damping = torch.exp(self.base_log_damping[level]).unsqueeze(0)  # [1, 6]
        
        # æ ¹æ®æ¢¯åº¦è‡ªé€‚åº”è°ƒæ•´
        adaptive_scale = self.damping_nets[level](gradient_stats)  # [B, 6]
        
        damping = base_damping * adaptive_scale  # [B, 6]
        
        # Clampåˆ°åˆç†èŒƒå›´
        damping = torch.clamp(damping, min=1e-4, max=10.0)
        
        return damping


class FeatureRefinementHead(nn.Module):
    """
    ä¸“é—¨ç”¨äºrefinementçš„è½»é‡çº§ç‰¹å¾æå–å¤´
    å°†åŸå§‹featuresè½¬æ¢ä¸ºæ›´é€‚åˆalignmentçš„å•é€šé“heatmap
    
    ğŸ†• æ”¯æŒä»»æ„è¾“å…¥é€šé“æ•°ï¼ˆ1, 8, 16ç­‰ï¼‰
    """
    def __init__(self, in_channels=8, hidden_channels=32):
        super().__init__()
        
        self.in_channels = in_channels
        
        # ğŸ†• å¦‚æœè¾“å…¥æ˜¯1é€šé“ï¼Œå…ˆæ‰©å±•é€šé“
        if in_channels == 1:
            self.channel_expander = nn.Sequential(
                nn.Conv2d(1, 8, 3, padding=1),
                nn.BatchNorm2d(8),
                nn.ReLU(inplace=True),
                nn.Conv2d(8, hidden_channels, 3, padding=1),
                nn.BatchNorm2d(hidden_channels),
                nn.ReLU(inplace=True)
            )
            # è·³è¿‡ conv1ï¼Œç›´æ¥ä» conv2 å¼€å§‹
            self.use_expander = True
        else:
            self.channel_expander = None
            self.use_expander = False
            
            # åŸæœ‰çš„ conv1ï¼ˆç”¨äºå¤šé€šé“è¾“å…¥ï¼‰
            self.conv1 = nn.Sequential(
                nn.Conv2d(in_channels, hidden_channels, 3, padding=1),
                nn.BatchNorm2d(hidden_channels),
                nn.ReLU(inplace=True)
            )
        
        # åç»­å±‚ä¿æŒä¸å˜ï¼ˆå¯¹ä¸¤ç§æƒ…å†µéƒ½é€‚ç”¨ï¼‰
        self.conv2 = nn.Sequential(
            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),
            nn.BatchNorm2d(hidden_channels),
            nn.ReLU(inplace=True)
        )
        
        self.conv3 = nn.Sequential(
            nn.Conv2d(hidden_channels, hidden_channels // 2, 3, padding=1),
            nn.BatchNorm2d(hidden_channels // 2),
            nn.ReLU(inplace=True)
        )
        
        self.output = nn.Conv2d(hidden_channels // 2, 1, 1)
    
    def forward(self, features):
        """
        Args:
            features: [B, C, H, W] åŸå§‹ç‰¹å¾ï¼ˆCå¯ä»¥æ˜¯1æˆ–8ç­‰ï¼‰
        
        Returns:
            refined_features: [B, 1, H, W] refinementä¸“ç”¨çš„å•é€šé“ç‰¹å¾
        """
        # ğŸ†• æ ¹æ®è¾“å…¥é€šé“æ•°é€‰æ‹©è·¯å¾„
        if self.use_expander:
            # è·¯å¾„1ï¼š1é€šé“ â†’ æ‰©å±•åˆ°hidden_channels
            x = self.channel_expander(features)
        else:
            # è·¯å¾„2ï¼šå¤šé€šé“ â†’ conv1
            x = self.conv1(features)
        
        # ä¸¤æ¡è·¯å¾„æ±‡åˆï¼Œåç»­å¤„ç†ç›¸åŒ
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.output(x)
        return x


class DifferentiablePoseRefinement(nn.Module):
    """
    å®Œå…¨å¯å¾®çš„ä½å§¿ä¼˜åŒ–æ¨¡å—
    ä½¿ç”¨ç®€åŒ–çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•è€Œä¸æ˜¯LMï¼Œç¡®ä¿ç«¯åˆ°ç«¯å¯è®­ç»ƒ
    """
    def __init__(self, num_levels=3, max_iterations=5, feature_dim=8):
        super().__init__()
        self.max_iterations = max_iterations
        self.num_levels = num_levels
        
        # 1. ç½®ä¿¡åº¦é¢„æµ‹å™¨
        self.confidence_predictor = ConfidencePredictor(feature_dim)
        
        # 2. è‡ªé€‚åº”dampingç½‘ç»œ
        self.damping_network = AdaptiveDampingNetwork(num_levels)
        
        # 3. Refinementä¸“ç”¨ç‰¹å¾å¤´
        self.feature_head = FeatureRefinementHead(
            in_channels=feature_dim,
            hidden_channels=32
        )
        
        # 4. å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ¯ä¸ªlevelå¯ä»¥ä¸åŒï¼‰
        self.lr_scale = nn.ParameterList([
            nn.Parameter(torch.ones(1) * 0.1)
            for _ in range(num_levels)
        ])
        
    def project_points(self, pose, points_3d, intrinsics):
        """
        å¯å¾®çš„3Dç‚¹æŠ•å½±
        
        Args:
            pose: [B, 4, 4] camera-to-world pose
            points_3d: [B, N, 3] ä¸–ç•Œåæ ‡ç³»çš„3Dç‚¹
            intrinsics: [B, 3, 3] å†…å‚çŸ©é˜µ
        
        Returns:
            points_2d: [B, N, 2] æŠ•å½±çš„2Dç‚¹
            depth: [B, N] æ·±åº¦
        """
        B, N, _ = points_3d.shape
        
        # å¤„ç†intrinsics
        if intrinsics.dim() == 2:
            intrinsics = intrinsics.unsqueeze(0).expand(B, -1, -1)
        if intrinsics.shape[-1] == 4:
            intrinsics = intrinsics[:, :3, :3]
        intrinsics = intrinsics.to(dtype=pose.dtype)
        
        # ä¸–ç•Œåæ ‡ -> ç›¸æœºåæ ‡
        pose_w2c = torch.inverse(pose)
        points_homo = torch.cat([
            points_3d,
            torch.ones(B, N, 1, device=points_3d.device, dtype=points_3d.dtype)
        ], dim=-1)
        
        points_cam = torch.bmm(
            pose_w2c[:, :3, :],
            points_homo.transpose(1, 2)
        ).transpose(1, 2)  # [B, N, 3]
        
        # æŠ•å½±åˆ°å›¾åƒå¹³é¢
        depth = points_cam[..., 2]  # [B, N]
        points_2d_homo = torch.bmm(
            intrinsics,
            points_cam.transpose(1, 2)
        )  # [B, 3, N]
        
        points_2d = points_2d_homo[:, :2, :] / (points_2d_homo[:, 2:3, :] + 1e-8)
        points_2d = points_2d.transpose(1, 2)  # [B, N, 2]
        
        return points_2d, depth
    
    def sample_features(self, features, points_2d, image_size):
        """
        å¯å¾®çš„ç‰¹å¾é‡‡æ ·
        
        Args:
            features: [B, C, H, W] ç‰¹å¾å›¾
            points_2d: [B, N, 2] 2Dç‚¹åæ ‡
            image_size: (H, W)
        
        Returns:
            sampled: [B, C, N] é‡‡æ ·çš„ç‰¹å¾
            valid_weights: [B, N] è½¯åŒ–çš„æœ‰æ•ˆæ€§æƒé‡
        """
        B, C, H, W = features.shape
        N = points_2d.shape[1]
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        grid = points_2d.clone()
        grid[..., 0] = 2.0 * grid[..., 0] / (W - 1) - 1.0
        grid[..., 1] = 2.0 * grid[..., 1] / (H - 1) - 1.0
        grid = grid.view(B, 1, N, 2)
        
        # å¯å¾®é‡‡æ ·
        sampled = F.grid_sample(
            features, grid,
            mode='bilinear',
            padding_mode='zeros',
            align_corners=True
        )  # [B, C, 1, N]
        
        sampled = sampled.squeeze(2)  # [B, C, N]
        
        # è®¡ç®—è½¯åŒ–çš„æœ‰æ•ˆæ€§æƒé‡ (è¾¹ç•Œé™„è¿‘å¹³æ»‘è¡°å‡)
        interpolation_pad = 4
        
        # ä½¿ç”¨sigmoidå®ç°è½¯è¾¹ç•Œ
        u = points_2d[..., 0]  # [B, N]
        v = points_2d[..., 1]
        
        u_valid = torch.sigmoid((u - interpolation_pad)) * \
                  torch.sigmoid((W - interpolation_pad - 1 - u))
        v_valid = torch.sigmoid((v - interpolation_pad)) * \
                  torch.sigmoid((H - interpolation_pad - 1 - v))
        
        valid_weights = u_valid * v_valid  # [B, N]
        
        return sampled, valid_weights
    
    def compute_alignment_score(self, pose, points_3d, features, intrinsics, image_size):
        """
        è®¡ç®—å¯¹é½åˆ†æ•°ï¼ˆç”¨äºä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ï¼‰
        
        Returns:
            score: [B] å¯¹é½åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            valid_ratio: [B] æœ‰æ•ˆç‚¹çš„æ¯”ä¾‹
        """
        B = pose.shape[0]
        
        # æŠ•å½±
        points_2d, depth = self.project_points(pose, points_3d, intrinsics)
        
        # æ·±åº¦æœ‰æ•ˆæ€§ï¼ˆè½¯åŒ–ï¼‰
        depth_valid = torch.sigmoid((depth - 0.1) * 10)  # [B, N]
        
        # é‡‡æ ·ç‰¹å¾
        sampled, spatial_valid = self.sample_features(features, points_2d, image_size)
        
        # æ€»æœ‰æ•ˆæ€§æƒé‡
        valid_weights = depth_valid * spatial_valid  # [B, N]
        
        # å¯¹é½åˆ†æ•°ï¼šåŠ æƒå¹³å‡çš„ç‰¹å¾å“åº”
        # å‡è®¾featuresæ˜¯alignment heatmapï¼Œå€¼è¶Šå¤§è¶Šå¥½
        if sampled.shape[1] == 1:
            score = (sampled.squeeze(1) * valid_weights).sum(dim=-1) / (valid_weights.sum(dim=-1) + 1e-6)
        else:
            # å¤šé€šé“ï¼šå–æœ€å¤§å“åº”
            score = (sampled.max(dim=1)[0] * valid_weights).sum(dim=-1) / (valid_weights.sum(dim=-1) + 1e-6)
        
        valid_ratio = valid_weights.mean(dim=-1)  # [B]
        
        return score, valid_ratio
    
    def rodrigues_rotation(self, axis_angle):
        """
        Rodrigueså…¬å¼ï¼šè½´è§’ -> æ—‹è½¬çŸ©é˜µ (å¯å¾®)
        
        Args:
            axis_angle: [B, 3]
        
        Returns:
            R: [B, 3, 3]
        """
        B = axis_angle.shape[0]
        device = axis_angle.device
        dtype = axis_angle.dtype
        
        angle = torch.norm(axis_angle, dim=-1, keepdim=True)  # [B, 1]
        
        # å¤„ç†å°è§’åº¦æƒ…å†µ
        small_angle_mask = (angle < 1e-8).squeeze(-1)
        angle_safe = torch.where(angle < 1e-8, torch.ones_like(angle) * 1e-8, angle)
        
        axis = axis_angle / angle_safe  # [B, 3]
        
        # æ„é€ åå¯¹ç§°çŸ©é˜µ K
        K = torch.zeros(B, 3, 3, device=device, dtype=dtype)
        K[:, 0, 1] = -axis[:, 2]
        K[:, 0, 2] = axis[:, 1]
        K[:, 1, 0] = axis[:, 2]
        K[:, 1, 2] = -axis[:, 0]
        K[:, 2, 0] = -axis[:, 1]
        K[:, 2, 1] = axis[:, 0]
        
        # Rodrigueså…¬å¼: R = I + sin(Î¸)K + (1-cos(Î¸))KÂ²
        I = torch.eye(3, device=device, dtype=dtype).unsqueeze(0).expand(B, -1, -1)
        angle_expanded = angle.unsqueeze(-1)  # [B, 1, 1]
        
        K_squared = torch.bmm(K, K)
        R = I + torch.sin(angle_expanded) * K + (1 - torch.cos(angle_expanded)) * K_squared
        
        # å°è§’åº¦æ—¶è¿”å›å•ä½çŸ©é˜µ
        R = torch.where(small_angle_mask.view(B, 1, 1), I, R)
        
        return R
    
    def apply_pose_delta(self, pose, delta):
        """
        åº”ç”¨6-DoFå¢é‡åˆ°pose (å¯å¾®)
        
        Args:
            pose: [B, 4, 4]
            delta: [B, 6] [delta_t, delta_r]
        
        Returns:
            updated_pose: [B, 4, 4]
        """
        delta_t = delta[:, :3]  # [B, 3]
        delta_r = delta[:, 3:]  # [B, 3]
        
        # è®¡ç®—å¢é‡æ—‹è½¬çŸ©é˜µ
        delta_R = self.rodrigues_rotation(delta_r)  # [B, 3, 3]
        
        # æå–å½“å‰poseçš„Rå’Œt
        R = pose[:, :3, :3]
        t = pose[:, :3, 3]
        
        # å·¦ä¹˜æ›´æ–°ï¼ˆåœ¨worldåæ ‡ç³»ä¸‹ï¼‰
        R_new = torch.bmm(delta_R, R)
        t_new = torch.bmm(delta_R, t.unsqueeze(-1)).squeeze(-1) + delta_t
        
        # æ„é€ æ–°pose
        pose_new = pose.clone()
        pose_new[:, :3, :3] = R_new
        pose_new[:, :3, 3] = t_new
        
        return pose_new
    
    def refine_step(self, pose, points_3d, features, intrinsics, image_size, level):
        """
        å•æ­¥refinement (å¯å¾®)
        
        ä½¿ç”¨æ¢¯åº¦ä¸Šå‡æ¥æœ€å¤§åŒ–å¯¹é½åˆ†æ•°
        """
        # 1. è®¡ç®—å½“å‰å¯¹é½åˆ†æ•°
        pose.requires_grad_(True)
        score, valid_ratio = self.compute_alignment_score(
            pose, points_3d, features, intrinsics, image_size
        )
        
        # 2. è®¡ç®—æ¢¯åº¦
        loss = -score.mean()  # è´Ÿå·ï¼šæ¢¯åº¦ä¸Šå‡
        
        # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
        if pose.grad is not None:
            pose.grad.zero_()
        
        loss.backward(retain_graph=True)
        
        if pose.grad is None:
            return torch.zeros(pose.shape[0], 6, device=pose.device), score.detach(), valid_ratio.detach()
        
        grad = pose.grad.clone()
        pose.requires_grad_(False)
        
        # 3. æå–6-DoFæ¢¯åº¦
        grad_t = grad[:, :3, 3]  # [B, 3]
        
        # æ—‹è½¬æ¢¯åº¦ï¼šå–åå¯¹ç§°éƒ¨åˆ†
        grad_R = grad[:, :3, :3]
        grad_R_skew = (grad_R - grad_R.transpose(-1, -2)) / 2
        grad_r = torch.stack([
            grad_R_skew[:, 2, 1],
            grad_R_skew[:, 0, 2],
            grad_R_skew[:, 1, 0]
        ], dim=-1)
        
        grad_6dof = torch.cat([grad_t, grad_r], dim=-1)  # [B, 6]
        
        # 4. è®¡ç®—adaptive damping
        grad_stats = torch.abs(grad_6dof)  # [B, 6]
        damping = self.damping_network(level, grad_stats)  # [B, 6]
        
        # 5. è®¡ç®—æ›´æ–°æ­¥é•¿
        lr = torch.abs(self.lr_scale[level])  # ç¡®ä¿æ­£å€¼
        delta = -lr * grad_6dof / (damping + 1e-6)  # [B, 6]
        
        # 6. Clipæ›´æ–°å¹…åº¦é˜²æ­¢å‘æ•£
        delta = torch.clamp(delta, -0.5, 0.5)
        
        return delta, score.detach(), valid_ratio.detach()
    
    def forward(self, pose, features, points_3d, intrinsics, image_size, 
                level, prob_distribution=None, training=True):
        """
        ç«¯åˆ°ç«¯å¯å¾®çš„refinement
        
        Args:
            pose: [B, 4, 4] åˆå§‹pose
            features: [B, C, H, W] ç‰¹å¾å›¾
            points_3d: [B, N, 3] 3Dç‚¹ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰
            intrinsics: [B, 3, 3/4] ç›¸æœºå†…å‚
            image_size: (H, W)
            level: stageç´¢å¼•
            prob_distribution: [B, N_samples] æŠ•ç¥¨æ¦‚ç‡åˆ†å¸ƒï¼ˆå¯é€‰ï¼‰
            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼
        
        Returns:
            refined_pose: [B, 4, 4] ä¼˜åŒ–åçš„pose
            aux_outputs: dict åŒ…å«è¾…åŠ©ä¿¡æ¯
        """
        B = pose.shape[0]
        
        # 1. æå–refinementä¸“ç”¨ç‰¹å¾
        refine_features = self.feature_head(features)  # [B, 1, H, W]
        
        # 2. é¢„æµ‹ç½®ä¿¡åº¦
        if prob_distribution is not None:
            confidence = self.confidence_predictor(
                features, pose, prob_distribution
            )  # [B, 1]
        else:
            # å¦‚æœæ²¡æœ‰prob_distributionï¼Œä½¿ç”¨é»˜è®¤å€¼
            confidence = torch.ones(B, 1, device=pose.device) * 0.5
        
        # 3. è¿­ä»£refinement
        current_pose = pose.detach().clone()  # æ–­å¼€æ¢¯åº¦ï¼Œfresh start
        
        iteration_info = {
            'deltas': [],
            'scores': [],
            'valid_ratios': []
        }
        
        for it in range(self.max_iterations):
            # æ‰§è¡Œä¸€æ­¥refinement
            delta, score, valid_ratio = self.refine_step(
                current_pose, points_3d, refine_features, 
                intrinsics, image_size, level
            )
            
            # åº”ç”¨æ›´æ–°
            current_pose = self.apply_pose_delta(current_pose, delta)
            
            # è®°å½•ä¿¡æ¯
            iteration_info['deltas'].append(delta)
            iteration_info['scores'].append(score)
            iteration_info['valid_ratios'].append(valid_ratio)
            
            # æ—©åœï¼ˆä»…æ¨ç†æ—¶ï¼‰
            if not training:
                delta_norm = torch.norm(delta, dim=-1).mean()
                if delta_norm < 1e-4:
                    break
        
        # 4. ä½¿ç”¨confidenceè¿›è¡ŒåŠ æƒblend
        # é«˜ç½®ä¿¡åº¦æ—¶æ›´å¤šä½¿ç”¨refined poseï¼Œä½ç½®ä¿¡åº¦æ—¶ä¿ç•™åŸå§‹pose
        confidence_weight = confidence.view(-1, 1, 1)
        blended_pose = confidence_weight * current_pose + (1 - confidence_weight) * pose
        
        # 5. å‡†å¤‡è¾“å‡º
        aux_outputs = {
            'confidence': confidence,  # [B, 1]
            'iteration_info': iteration_info,
            'refined_pose_raw': current_pose,  # blendä¹‹å‰çš„
            'refine_features': refine_features,
            'final_score': iteration_info['scores'][-1] if iteration_info['scores'] else None,
            'final_valid_ratio': iteration_info['valid_ratios'][-1] if iteration_info['valid_ratios'] else None
        }
        
        return blended_pose, aux_outputs


def test_learnable_refinement():
    """
    æµ‹è¯•ä»£ç 
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # åˆ›å»ºæ¨¡å—
    refiner = DifferentiablePoseRefinement(
        num_levels=3,
        max_iterations=5,
        feature_dim=8
    ).to(device)
    
    # æµ‹è¯•æ•°æ®
    B = 2
    pose = torch.eye(4).unsqueeze(0).repeat(B, 1, 1).to(device)
    pose[:, :3, 3] = torch.randn(B, 3).to(device) * 10  # éšæœºå¹³ç§»
    
    features = torch.randn(B, 8, 64, 64).to(device)
    points_3d = torch.randn(B, 100, 3).to(device) * 50
    intrinsics = torch.eye(3).unsqueeze(0).repeat(B, 1, 1).to(device)
    intrinsics[:, 0, 0] = 500  # fx
    intrinsics[:, 1, 1] = 500  # fy
    intrinsics[:, 0, 2] = 320  # cx
    intrinsics[:, 1, 2] = 240  # cy
    
    prob_dist = torch.randn(B, 100).softmax(dim=-1).to(device)
    
    # Forward
    refined_pose, aux = refiner(
        pose, features, points_3d, intrinsics, (64, 64),
        level=0, prob_distribution=prob_dist, training=True
    )
    
    print(f"Input pose shape: {pose.shape}")
    print(f"Refined pose shape: {refined_pose.shape}")
    print(f"Confidence: {aux['confidence']}")
    print(f"Final score: {aux['final_score']}")
    print(f"Test passed!")


if __name__ == '__main__':
    test_learnable_refinement()